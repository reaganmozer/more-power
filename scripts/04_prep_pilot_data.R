setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
options(stringsAsFactors = F)
setwd("../")



pilot = read.csv( "raw data/write_machinelearning_pilot_replication.csv",
                  encoding='WINDOWS-1252')


pilot <- pilot %>% mutate(ID=row_number())

# Drop empty responses
pilot <- pilot |> filter(response != "")

# Grab key columns of metadata
all.feats = select(pilot, ID, 
                   writing_quality_score_2, more)

# Convert text to UTF-8
pilot$response <- iconv(pilot$response, from='WINDOWS-1252', to='ASCII', sub=" ")

# Get text (and repair one piece of spelling)
pilot$response =  rcttext::repair_spelling(pilot$response, "shoud", "should" )


# Save intermediate file for processing in LIWC
dat = pilot %>% rename(score=writing_quality_score_2, text=response) %>% 
  select(ID, more, score, text)



dat = select(dat, ID, everything())


txt=stringi::stri_trim_right(dat$text)
txt2=textclean::replace_non_ascii(txt)
dat$text = iconv(txt2, from="UTF-8",to="ASCII",sub="")

text = tm::stripWhitespace(dat$text)



all.feats = dat %>% select( ID, more, score) %>% rename( Z=more)

# 2. Make large set of features for the text corpus ----

# Create preliminary text features
feats = rcttext::generate_features( dat$text,
                                    meta = all.feats,
                                    sent = TRUE,
                                    clean_features = FALSE,
                                    read = c("Flesch","Flesch.Kincaid", "ARI"),
                                    ld=c("TTR","R","K"),
                                    ignore=c("docID","s_id","ID"),
                                    verbose = TRUE )

# Load LIWC-generated features
# (This loads file generated by LIWC program and cleans it)
liwc=read.csv("external data/write-pilot-liwc22.csv")
all.feats = merge(feats,liwc,by="ID")


if (FALSE){ #Use the code below to generate text embeddings using OpenAI embed-3-small model
  
  # Replace the text below with your unique API token
  API_KEY = "ENTER API TOKEN"
  Sys.setenv(OPENAI_API_KEY=API_KEY)
  
  library(openai)
  
  emb.pilot = openai::create_embedding(model="text-embedding-3-small", input=dat$text)
  out = do.call(rbind, emb.pilot$data$embedding)
  embedding.pilot = out
  save(embedding.pilot, file="external data/write-pilot-embed-3-small.RData")
  
  
}


# Add PCA reduction of OpenAI embeddings
load("external data/write-pilot-embed-3-small.RData")
pca = prcomp(embedding.pilot)
X.pc = as.data.frame(pca$x[,1:50])
names(X.pc)=paste("t3s.PC",1:ncol(X.pc),sep="")


all.feats = cbind(all.feats, X.pc)

# Add output from TAACO
tc = read.csv("external data/write-pilot-taaco2.1.3.csv")
tc$ID=as.numeric(gsub(".txt","",tc$Filename))
tc = select(tc, ID, everything(), -Filename)
tc = tc %>% arrange(ID)

all.feats = merge(all.feats,tc,by=c("ID"))


dim(all.feats)
table( apply(all.feats, 2, anyNA) )

all.info = all.feats

names(all.info) = gsub( "lex_f.ent[, -c(1)]","lex_f.ent", names(all.info), fixed=T )
names(all.info) = gsub( " ", "_", names(all.info), fixed=T )

X = all.info %>% select(-any_of(c("s_id","docID","score","Z" )))
lc=caret::findLinearCombos(X)

X = X[,-c(lc$remove)]
nz = caret::nearZeroVar(X, uniqueCut=0.5,freqCut=500/1,names=F)
X = X[,-c(nz)]

all.pilot = select(all.info, ID, score, Z, names(X))

save(all.pilot, file="generated data/MORE-pilot.RData")


